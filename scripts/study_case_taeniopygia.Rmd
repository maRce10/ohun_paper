---
title: <center><font size="6"><b>Energy-based automatic detection on zebra finch songs</b></font></center>
subtitle: <center><font size="4"><b>ohun</b></font></center>
author: <center><font size="4"><a href="http://marceloarayasalas.weebly.com/">Marcelo Araya-Salas, PhD</a></font></center>
date: <center>`r format(Sys.Date(), "%d-%m-%Y")`</center>
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: no
fontsize: 12pt 
editor_options: 
  chunk_output_type: console
---

&nbsp;

<div class="alert alert-info">

### Purpose

- Showcase energy-based automatic detection with the R package ohun using zebra-finch songs

</div>

&nbsp;


```{r setup, include = FALSE}

knitr::opts_knit$set(root.dir = normalizePath(".."))

knitr::opts_chunk$set(include = TRUE, eval = TRUE)

data_path <- raw_data_path <- "data/raw/taeniopygia"

preproc_path <- "data/processed/taeniopygia"

figure_path <- "data/processed/taeniopygia/figures"

```

### Load package
```{r libraries, eval = TRUE, echo = FALSE, message=FALSE, warning=FALSE}

x <- c("ohun", "Rraven", "warbleR", "pbapply", "remotes", "DT")

out <- lapply(x, function(y) {
  
  # check if installed, if not then install 
  if (!y %in% installed.packages()[,"Package"]) 
    install.packages(y) 

  # load package
  try(require(y, character.only = T), silent = T)
})

```

```{r, message=FALSE, warning=FALSE}
library(ohun)

library(warbleR)

library(viridis)
```

```{r manual selection table, eval = FALSE, echo = FALSE}

#Read in manual selections made in Raven

mnl_sel_tbl <- imp_raven(path = raw_data_path, name.from.file = TRUE, ext.case = "lower", all.data = TRUE)

glimpse(mnl_sel_tbl)

mnl_sel_tbl <- mnl_sel_tbl %>% 
  dplyr::rename(
    `start` = "Begin Time (s)",
    `end` = "End Time (s)",
    `selec` = "Selection",
    `selection_notes` = "Annotation"
  ) %>% 
  dplyr::mutate(
    bottom.freq = 0.5,
    top.freq = 10
  ) %>% 
  dplyr::select(sound.files, selec, start, end, selec.file, bottom.freq, top.freq, selection_notes)

glimpse(mnl_sel_tbl)

# Calculate signal to noise ratio (SNR)
mnl_sel_tbl_snr <- sig2noise(mnl_sel_tbl, mar = 0.001, type = 1, in.dB = TRUE, before = FALSE, lim.dB = TRUE, bp = c(unique(mnl_sel_tbl$bottom.freq), unique(mnl_sel_tbl$top.freq)), wl = 10, path = raw_data_path)

glimpse(mnl_sel_tbl_snr)

# Write out the combined manual selections out as a selection table (.csv)
write.csv(mnl_sel_tbl_snr, file.path(preproc_path, "manual_selections_Taeniopygia.csv"), row.names = FALSE)

```

```{r manual extended selection table, eval = FALSE, echo = FALSE}
#Write out the combined manual selections out as an extended selection table (RDS).

est <- selection_table(mnl_sel_tbl_snr, whole.recs = FALSE, extended = TRUE, mar = 0.01, parallel = cores, confirm.extended = FALSE, path = raw_data_path)

glimpse(est)
class(est)

# Update sound file names, which now have "_number" after the ".wav" extension
wav_nms <- names(attr(est, "wave.objects"))
wav_nms <- paste(gsub(".wav", "", wav_nms), ".wav", sep = "")
head(wav_nms) # looks good

est2 <- rename_est_waves(est, wav_nms)

# Looks good
head(names(attr(est2, "wave.objects")))
glimpse(est2)

# Remove the "old.sound.file.name" column
est2 <- est2[, -grep("old.sound.file.name", names(est2))]
glimpse(est2)

# Add in original sound file name, since each sound.file value is currently a unique name per row of the table
check_df <- attr(est2, "check.res")
glimpse(check_df)
head(check_df$sound.files)

# Are all sampling rates and bit rates the same?
# Yes: 44.1 kHz sampling rate and bit rate of 16
check_df %>%
  distinct(sample.rate, bits)

# Does the order of selections match between the EST and this attribute data frame? If can, can do the column addition below of original sound file name, start and end coordinates, selection duration
all(est2$sound.files == check_df$sound.files)

est2$duration <- check_df$duration
est2$original_sound_files <- check_df$orig.sound.files
est2$original_start <- check_df$orig.start
est2$original_end <- check_df$orig.end

# Looks good
glimpse(est2)
class(est2)
length(attr(est2, "wave.objects"))

saveRDS(est2, file.path(preproc_path, "manual_selections_Taeniopygia_est.RDS"))

```

```{r automated detection parameters, eval = FALSE, echo = FALSE}

est <- readRDS(file.path(preproc_path, "manual_selections_Taeniopygia_est.RDS"))
# glimpse(est)

mnl_sel_tbl <- read.csv(file.path(preproc_path, "manual_selections_Taeniopygia.csv"))
# glimpse(mnl_sel_tbl)

# Use ohun::feature_reference to get parameters that can help automated detection
# ?ohun::feature_reference

# I get an error when using the EST: "# Error in warbleR::gaps(X = reference, pb = FALSE) : 
  # extended selection tables must be created 'by.song' to be used in song.param()"
# feat_ref <- ohun::feature_reference(est, by.sound.file = FALSE,)

feat_ref <- ohun::feature_reference(mnl_sel_tbl, by.sound.file = FALSE)

# Min and max duration of the manual selections
# Use these for automated detection 
min_dur <- floor(feat_ref[["sel.duration", "min"]]) 
max_dur <- ceiling(feat_ref[["sel.duration", "max"]] )
min_gap <- floor(feat_ref[["gap.duration", "min"]])
bp <- c(feat_ref[["bottom.freq", "min"]] - 0.5, feat_ref[["top.freq", "max"]] + 0.5)

min_dur
max_dur
min_gap
bp

# A single file for testing
wav <- "R39Y212_43533.30691176_3_9_8_31_31.wav"

```

&nbsp;

### Set directory where the sound files and annotations are found

```{r, eval = FALSE, echo = TRUE}

data_path <- "DIRECTORY_WITH_SOUND_FILES_AND_ANNOTATIONS_HERE"

```


&nbsp;

### Read reference annotations
```{r, eval = TRUE, echo = FALSE}

manual_ref <- read.csv(file.path("data/processed/taeniopygia", "manual_selections_Taeniopygia.csv"))

```

```{r, eval = FALSE, echo = TRUE}

manual_ref <- read.csv(file.path(data_path, "manual_selections_Taeniopygia.csv"))

```

&nbsp;

### Create spectrograms to explore vocalization structure

This code creates a multipanel image with multiple spectrograms, one for each of the individuals/recordings in the complete data set
```{r Create catalogs, eval = FALSE}

# select highest signal to noise ratio calls per individual
manual_ref_snr <- signal_2_noise(X = manual_ref, mar = 0.05, path = data_path)

# select 1 example per sound file
high_snr <- manual_ref_snr[ave(-manual_ref_snr$SNR, manual_ref_snr$sound.files, FUN = rank) <= 1, ]

# create catalogs
catalog(X = high_snr, flim = c(0, 11), nrow = 6, ncol = 3, ovlp = 90, height = 15, width = 20, same.time.scale = FALSE, mar = 0.01, wl = 512, gr = FALSE, spec.mar = 0.4, lab.mar = 0.8, rm.axes = TRUE, by.row = TRUE, box = TRUE, pal = viridis, parallel = 10, collevels = seq(-100, 0, 5), path = data_path)

```

```{r catalog, out.width = "100%", echo = FALSE, fig.align= "center"}

knitr::include_graphics(normalizePath(file.path(data_path, "Catalog_p1.jpeg")))

```

&nbsp;

### Split reference annotations for training and testing detection
```{r test and train, eval = TRUE, echo = TRUE}

train_files <- c(
  "BRN7_43435.27985312_12_1_7_46_25.wav",
  "db1_43427.28100313_11_23_7_48_20.wav",
  "Gry35HP_43455.29800260_12_21_8_16_40.wav"
)
test_files <- setdiff(manual_ref$sound.files, train_files)

train_ref <- manual_ref[manual_ref$sound.files %in% train_files, ]
test_ref <- manual_ref[manual_ref$sound.files %in% test_files, ]
```

&nbsp;

### Optimize detection on training data
```{r  optimize automated detection, eval = FALSE, echo = TRUE}

opt_det <- optimize_energy_detector(
  reference = train_ref, 
  files = train_files, 
  threshold = c(1, 5), 
  hop.size = 11.6, 
  smooth = c(5, 10), 
  hold.time = c(0, 5), 
  min.duration = c(5, 15, 25), 
  max.duration = c(275, 300, 325), 
  bp = c(0.5, 10),
  path = data_path
)

```

```{r  save optimized automated detection, eval = FALSE, echo = FALSE}

write.csv(opt_det, file.path(preproc_path, "detection_optimization.csv"), row.names = FALSE)
```

&nbsp;

### Check subset with highest performance (sorted by f1 score)
```{r, eval = FALSE}
# subset with highest performance
sub_opt_det <- opt_det[order(opt_det$f1.score, decreasing = TRUE), ]

sub_opt_det <- sub_opt_det[1:10, c("threshold", "smooth", "hold.time", "min.duration", "max.duration", "true.positives", "false.positives",  "false.negatives", "recall", "precision", "f1.score")]

#print 
sub_opt_det

```

```{r, eval = TRUE, echo = FALSE}

opt_det <- read.csv(file.path(preproc_path, "detection_optimization.csv"))

# subset with highest performance
sub_opt_det <- opt_det[order(opt_det$f1.score, decreasing = TRUE), ]

sub_opt_det <- sub_opt_det[1:10, c("threshold", "smooth", "hold.time", "min.duration", "max.duration", "true.positives", "false.positives",  "false.negatives", "recall", "precision", "f1.score")]

# print dynamic table
oa_DT <- datatable(sub_opt_det, editable = list(target = "row"), rownames = FALSE, style = "bootstrap",
    filter = "top", options = list(pageLength = 100, autoWidth = TRUE, dom = "ft"),
    autoHideNavigation = TRUE, escape = FALSE)

formatRound(table = oa_DT, columns = sapply(sub_opt_det, is.numeric), 3)
```

&nbsp;

### Select the highest performance parameters based on f1 score
```{r, select best params, eval = TRUE, echo = TRUE}

best_param <- opt_det[which.max(opt_det$f1.score), ]

best_param
```

&nbsp;

### Run detection on test data
```{r, detect on all data, eval = TRUE, echo = TRUE}

opt_det_all <- energy_detector(
  files = test_files, 
  threshold = best_param$threshold, 
  hop.size = 11.6, 
  smooth = best_param$smooth, 
  hold.time = best_param$hold.time, 
  min.duration = best_param$min.duration, 
  max.duration = best_param$max.duration, 
  bp = c(0.5, 10), 
  path = data_path
)
```

&nbsp;

### Evaluate detection performance
```{r, diagnose, eval = TRUE, echo = TRUE}

diagnose_detection(reference = test_ref, detection = opt_det_all, by.sound.file = FALSE)

```

&nbsp;

<div class="alert alert-info">

### Takeaways

- Good performance on detecting zebra finch songs: F1 score was 0.95 for the training data set and 0.96 for the testing data 

</div>

&nbsp;

---

<font size="4">Session information</font>

```{r session info, echo=F}

sessionInfo()

```
